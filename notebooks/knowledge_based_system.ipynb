{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d23422e-4447-418e-b305-418d28eced0b",
   "metadata": {},
   "source": [
    "# Knowledge Based System with Langchain and PaLM \n",
    "This notebook walks through building a question/answer system that retrieves information from a private knowledge base. A pre-trained LLM, or likely even a fine-tuned LLM will not be sufficient (in and of itself) when you want a conversational interface to ask specific questions about specific data (private knowledge base). This private knowledge base can be a collection of documents, websites, research papers, or even structured data tables and more. \n",
    "\n",
    "The steps to setup the private knowledge base are as follows:\n",
    "1) Split documents into chunks\n",
    "2) Vectorize (embed) each chunk \n",
    "3) Store vectors/embeddings in a database\n",
    "\n",
    "Once you have a vectorstore of embeddings (private knowledge-base), the process of using it in a conversational workflow are as follows:\n",
    "1) Embed the query (question)\n",
    "2) Nearest neighbors lookup with query in vectorstore to find relevant chunks\n",
    "3) Use relevant chunks to formulate response  \n",
    "\n",
    "This process of course requires an LLM (like PaLM or others) to formulate responses to queries with the relevant chunks found via nearest neighbors.  \n",
    "\n",
    "Of course there are many options for a vectorstore, including managed and scalable offerings like [Vertex AI Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview). Additionally there are different options for LLMs to use as the underpinning language model. In this walkthrough we will use [Chroma](https://www.trychroma.com/) as a vectorstore and [PaLM](https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/api-quickstart) as the underpinning language model. In a production environment, consider using a more scalable and efficient vector store such as Vertex AI Matching Engine. \n",
    "\n",
    "**NOTE:** This notebook requires you to have a Google Cloud project and uses Google Cloud resources. If you are not running this lab in a Vertex AI Workbench Notebook, you need to set up the proper permissions to access these resources. Help can be found [here](https://cloud.google.com/docs/authentication/provide-credentials-adc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a750d4-e74c-400f-ae2d-d11362816124",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee867b-c317-4f43-835c-324d64610f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --user \\\n",
    "    langchain==0.0.217 \\\n",
    "    wikipedia==1.4.0 \\\n",
    "    chromadb==0.3.26 \\\n",
    "    google-cloud-aiplatform==1.26.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad1d44-4275-4ddd-a6b7-ec68c9f0cd0a",
   "metadata": {},
   "source": [
    "Restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d49e9b-a846-4ff7-827d-2947b6f0b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f931c6-119c-49f0-a840-0eaa4484b4bd",
   "metadata": {},
   "source": [
    "### Document Loading \n",
    "Langchain provides classes to load data from different sources. Some useful data loaders are [Google Cloud Storage Directory Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/google_cloud_storage_directory), [Google Drive Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/google_drive), [Recursive URL Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/recursive_url_loader), [PDF Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/how_to/pdf), [JSON Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/how_to/json), [Wikipedia Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/wikipedia), and [more](https://python.langchain.com/docs/modules/data_connection/document_loaders/). \n",
    "\n",
    "In this notebook we will use the Wikipedia loader to create a private knowledge base of wikipedia articles about machine learning, but the overall process is similiar regardless of which document loader you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ac74b8-68cb-4f91-a6ce-a557eb24c471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines \\'discover\\' their \\'own\\' algorithms, without needing to be explicitly told what to do by any human-developed algorithms. When there was a vast amount of potential answers, the correct ones needed to be labeled as valid by human labelers initially and human supervision was need. With advance of faster machines and new methods, however,  \\'discovering\\' machine\\'s own models became possible not only by using supervised learning but also by using unsupervised learning or reinforcement learning. Although not all machine learning is statistically-based, computational statistics is an important source of the field\\'s methods. \\nGenerative artificial neural networks, mimicking the working of a biological brain, has been recently able to surpass results of many previous approaches.The applications include computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics.\\n\\n\\n== Basic assumption ==\\nIt is based on the basic assumption that whatever (i.e. strategies, algorithms, and inferences) worked in the past will most likely to continue working in the future, sometimes as obviously as in \"since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well\", however other times in more nuanced way as in \"X% of families have geographically separate species with color variants, so there is a Y% chance that undiscovered black swans exist\".\\n\\n\\n== History and relationships to other fields ==\\n\\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.By the early 1960s an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to re-evaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson\\'s book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".Modern-day machine learning has two objectives, one is to classify data based on models which have been developed, the other purpose is to make predictions for future outco', metadata={'title': 'Machine learning', 'summary': \"Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines 'discover' their 'own' algorithms, without needing to be explicitly told what to do by any human-developed algorithms. When there was a vast amount of potential answers, the correct ones needed to be labeled as valid by human labelers initially and human supervision was need. With advance of faster machines and new methods, however,  'discovering' machine's own models became possible not only by using supervised learning but also by using unsupervised learning or reinforcement learning. Although not all machine learning is statistically-based, computational statistics is an important source of the field's methods. \\nGenerative artificial neural networks, mimicking the working of a biological brain, has been recently able to surpass results of many previous approaches.The applications include computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics.\", 'source': 'https://en.wikipedia.org/wiki/Machine_learning'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = WikipediaLoader(query=\"Machine Learning\", load_max_docs=10).load()\n",
    "docs += WikipediaLoader(query=\"Deep Learning\", load_max_docs=10).load() \n",
    "docs += WikipediaLoader(query=\"Neural Networks\", load_max_docs=10).load() \n",
    "\n",
    "# Take a look at a single document\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c614796-f3cb-447a-9a6a-5655b31f2289",
   "metadata": {},
   "source": [
    "### Split text into chunks\n",
    "Now that we have the documents we will split them into chunks. Each chunk will become one vector in the vector store. To do this we will define a chunk size (number of characters) and a chunk overlap (amount of overlap i.e. sliding window). The perfect chunk size can be difficult to determine. Too large of a chunk size leads to too much information per chunk (individual chunks not specific enough), however too small of a chunk size leads to not enough information per chunk. In both cases, nearest neighbors lookup with a query/question embedding may struggle to retrieve the actually relevant chunks, or fail altogether if the chunks are too large to use as context with an LLM query.\n",
    "\n",
    "In this notebook we will use a chunk size of 800 chacters and a chunk overlap of 400 characters, but feel free to experiment with other sizes! Note: you can specify a custom `length_function` with `RecursiveCharacterTextSplitter` if you want chunk size/overlap to be determined by something other than Python's `len` function. In addition to `RecursiveCharacterTextSplitter`, there are [other text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/split_by_token) you can consider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4bf2159-7ecd-4b9f-8fc2-9b66e4b1f050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines 'discover' their 'own' algorithms, without needing to be explicitly told what to do by any human-developed algorithms. When there was a vast amount of potential answers, the correct ones needed to be labeled as valid by human labelers initially and human supervision was need. With advance of faster machines and new methods, however,  'discovering' machine's own models became possible not only by using supervised learning but also by using unsupervised learning or reinforcement learning. Although not all machine learning is statistically-based, computational statistics is an important source\", metadata={'title': 'Machine learning', 'summary': \"Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines 'discover' their 'own' algorithms, without needing to be explicitly told what to do by any human-developed algorithms. When there was a vast amount of potential answers, the correct ones needed to be labeled as valid by human labelers initially and human supervision was need. With advance of faster machines and new methods, however,  'discovering' machine's own models became possible not only by using supervised learning but also by using unsupervised learning or reinforcement learning. Although not all machine learning is statistically-based, computational statistics is an important source of the field's methods. \\nGenerative artificial neural networks, mimicking the working of a biological brain, has been recently able to surpass results of many previous approaches.The applications include computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics.\", 'source': 'https://en.wikipedia.org/wiki/Machine_learning'}),\n",
       " Document(page_content=\"labeled as valid by human labelers initially and human supervision was need. With advance of faster machines and new methods, however,  'discovering' machine's own models became possible not only by using supervised learning but also by using unsupervised learning or reinforcement learning. Although not all machine learning is statistically-based, computational statistics is an important source of the field's methods.\", metadata={'title': 'Machine learning', 'summary': \"Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines 'discover' their 'own' algorithms, without needing to be explicitly told what to do by any human-developed algorithms. When there was a vast amount of potential answers, the correct ones needed to be labeled as valid by human labelers initially and human supervision was need. With advance of faster machines and new methods, however,  'discovering' machine's own models became possible not only by using supervised learning but also by using unsupervised learning or reinforcement learning. Although not all machine learning is statistically-based, computational statistics is an important source of the field's methods. \\nGenerative artificial neural networks, mimicking the working of a biological brain, has been recently able to surpass results of many previous approaches.The applications include computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.The mathematical foundations of ML are provided by mathematical optimization methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning.ML is known in its application across business problems under the name predictive analytics.\", 'source': 'https://en.wikipedia.org/wiki/Machine_learning'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 400,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Look at the first two chunks \n",
    "chunks[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b104016c-4405-438f-88f2-5344697a6a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 30\n",
      "Number of chunks: 258\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of documents: {len(docs)}')\n",
    "print(f'Number of chunks: {len(chunks)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ade88-2087-4b55-bbf5-dcd6b4213e02",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vectorize/Embed Document Chunks\n",
    "Now we need to embed the document chunks (turn them into vectors) and store them in a vectorstore. For this, we can use any text embedding model, however we need to be sure to use the same text embedding model when we embed our queries/questions at prediction time. To make things simple we will use the PaLM API for Embeddings. The langchain library provides a nice wrapper class around the PaLM Embeddings API, `VertexAIEmbeddings()`.\n",
    "\n",
    "Since Vertex AI Matching Engine takes awhile (~45 minutes) to create an index, we will use [Chroma](https://www.trychroma.com/) instead to keep things simple. Of course, in a real-world use case with a large private knowledge-base, you may not be able to fit everything in memory. Langchain has a nice wrapper class for Chroma which allows us to pass in a list of documents, and an embedding class to create the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca86eea9-5055-4e30-a659-9440bfa235ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = VertexAIEmbeddings() # PaLM embedding API \n",
    "\n",
    "# set persist directory so the vector store is saved to disk\n",
    "db = Chroma.from_documents(chunks, embedding, persist_directory=\"./vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed88bb-86d4-42ac-878f-cf81d7723d41",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "Now that everything is in place, we can tie it all together with a langchain chain. A langchain chain simply orchestrates the multiple steps required to use an LLM for a specific use case. In this case the process we will chain together first embeds the query/question, then performs a nearest neighbors lookup to find the relevant chunks, then uses the relevant chunks to formulate a response with an LLM. We will use the Chroma database as our vector store and PaLM as our LLM. Langchain provides a wrapper around PaLM, `VertexAI()`. \n",
    "\n",
    "For this simple Q/A use case we can use langchain's `RetrievalQA` to link together the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d30b4e89-4a2b-4a3d-ab47-3b2a53c5f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector store \n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":5} # number of nearest neighbors to retrieve  \n",
    ")\n",
    "\n",
    "# PaLM API \n",
    "# You can also set temperature, top_p, top_k \n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison\",\n",
    "    max_output_tokens=1024\n",
    ")\n",
    "\n",
    "# q/a chain \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6086001-b494-41bd-9372-494edb95ea2e",
   "metadata": {},
   "source": [
    "### Query \n",
    "Now that everything is tied together we can send queries and get answers! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fbbc0c9-6955-4ba3-9587-62b7e32215f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str):\n",
    "    response = qa({\"query\": question})\n",
    "    print(f\"Response: {response['result']}\\n\")\n",
    "\n",
    "    citations = {doc.metadata['source'] for doc in response['source_documents']}\n",
    "    print(f\"Citations: {citations}\\n\")\n",
    "\n",
    "    # uncomment below to print source chunks used  \n",
    "    # print(f\"Source Chunks Used: {response['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f937a6e-a9cd-4a0f-b632-cddab0a44ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The technology that underpins large language models is the transformer architecture.\n",
      "\n",
      "Citations: {'https://en.wikipedia.org/wiki/Recurrent_neural_network', 'https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"What technology underpins large language models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0272b2b-7706-4bc8-9648-8c9eb1de2805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Gradient boosted trees (GBTs) are a type of ensemble machine learning algorithm that combines decision trees with gradient boosting. Gradient boosting is an iterative process that builds a model by adding new trees to an existing model in order to reduce the error of the model. The first tree is built using the original data, and then subsequent trees are built using the residuals from the previous tree. This process is repeated until the desired level of accuracy is achieved. GBTs are often used for classification and regression tasks, and they are particularly well-suited for tasks where the data is noisy or incomplete.\n",
      "\n",
      "Citations: {'https://en.wikipedia.org/wiki/Support_vector_machine', 'https://en.wikipedia.org/wiki/Rectifier_(neural_networks)', 'https://en.wikipedia.org/wiki/Graph_neural_network', 'https://en.wikipedia.org/wiki/Boosting_(machine_learning)'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"What is a gradient boosted tree?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48e118cd-9a90-429b-97f8-5f5d516645d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The Transformer model came out in 2017.\n",
      "\n",
      "Citations: {'https://en.wikipedia.org/wiki/Artificial_neural_network', 'https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)', 'https://en.wikipedia.org/wiki/Recurrent_neural_network'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"When was the transformer invented?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b81c6-c5aa-4703-8e79-d10b96c68bb1",
   "metadata": {},
   "source": [
    "### Preserve Chat History\n",
    "`RetrievalQA` is great for asking single questions and getting an answer, but want if you want a chatbot that is able to track conversation history and understand context within a conversation? For that, we can use `ConversationalRetrievalChain` to orchestrate the flow (similar to `RetrievalQA`) and `ConversationBufferMemory` to preserve chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3befc621-f236-428a-90c3-50eb00bc6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserve chat history in memory \n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chat_session = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=retriever, \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd297dd9-9a4b-4e7c-ae88-a90748b68489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What technology underpins large language models?',\n",
       " 'chat_history': [HumanMessage(content='What technology underpins large language models?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The technology that underpins large language models is the transformer architecture.', additional_kwargs={}, example=False)],\n",
       " 'answer': 'The technology that underpins large language models is the transformer architecture.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_session({'question': 'What technology underpins large language models?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de2d2a24-1ce1-432b-a0ca-77ad84752f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'When were they invented?',\n",
       " 'chat_history': [HumanMessage(content='What technology underpins large language models?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The technology that underpins large language models is the transformer architecture.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='When were they invented?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The Transformer model came out in 2017.', additional_kwargs={}, example=False)],\n",
       " 'answer': 'The Transformer model came out in 2017.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With chat history it will understand that \"they\" refers to transformers \n",
    "chat_session({'question': 'When were they invented?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401536c8-b1b8-4727-adde-b24e51274f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
